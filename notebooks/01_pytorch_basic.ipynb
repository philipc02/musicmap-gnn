{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f93dfe",
   "metadata": {},
   "source": [
    "# PyTorch Basics: Day 1\n",
    "Goal: Get comfortable with PyTorch tensors, basic operations, and a simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a03ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition: tensor([3., 7.])\n",
      "Dot product: tensor(14.)\n",
      "dy/dx: tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Basics\n",
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "a = torch.tensor([2.0, 3.0])\n",
    "b = torch.tensor([1.0, 4.0])\n",
    "print(\"Addition:\", a + b)\n",
    "print(\"Dot product:\", torch.dot(a, b))\n",
    "\n",
    "# Gradients (autograd)\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2 + 3*x\n",
    "y.backward()    # computes derivative using automatic differentiation\n",
    "print(\"dy/dx:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbff544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 0.6840\n",
      "Epoch 1: loss = 0.6816\n",
      "Epoch 2: loss = 0.6795\n",
      "Epoch 3: loss = 0.6776\n",
      "Epoch 4: loss = 0.6764\n",
      "Epoch 5: loss = 0.6755\n",
      "Epoch 6: loss = 0.6747\n",
      "Epoch 7: loss = 0.6742\n",
      "Epoch 8: loss = 0.6737\n",
      "Epoch 9: loss = 0.6735\n"
     ]
    }
   ],
   "source": [
    "# Simple PyTorch Neural Network\n",
    "import torch.nn as nn   # for building neural networks\n",
    "import torch.optim as optim # for optimization\n",
    "\n",
    "# Dummy dataset\n",
    "X = torch.rand((10, 2)) # 10 samples each with 2 random features/characteristics (f.ex. 10 songs with 2 features per song: tempo, likeability score -> scores between 0 and 1 generated using .rand())\n",
    "y = torch.randint(0, 2, (10,))  # 10 labels (either 0 or 1), randomly assigned\n",
    "\n",
    "# Model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 4),    # fully connected layer: 2 inputs, 4 outputs\n",
    "    nn.ReLU(),  # activation function that makes outputs non-linear\n",
    "    nn.Linear(4, 2) # final layer, 4 inputs, 2 outputs (for class 0 and class 1)\n",
    ")   # creates small feedforward NN -> 2-layer NN (MLP = multi layer perceptron)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()   # loss function (cross entropy loss used for classification)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) # Adam -> gradient descent optimizer for model's weights (uses computed gradients)\n",
    "\n",
    "# Training loop (just 10 steps)\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()   # resets gradients from previous iteration (otherwise gradients from mutliple backward passes will add up -> like this only the current gradient is used to update the weights)\n",
    "    outputs = model(X)  # forward pass: predict from X\n",
    "    loss = criterion(outputs, y)    # compute loss between prediction and actual\n",
    "    loss.backward() # compute gradients (backward pass)\n",
    "    optimizer.step()    # update weights (model parameters) using gradients    \n",
    "    print(f\"Epoch {epoch}: loss = {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
